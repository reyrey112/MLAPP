# from pipelines.deployment_pipeline import *
# import click
# from rich import print
# from typing import cast

# from zenml.integrations.mlflow.mlflow_utils import get_tracking_uri
# from zenml.integrations.mlflow.model_deployers.mlflow_model_deployer import (
#     MLFlowModelDeployer,
# )
# from zenml.integrations.mlflow.services.mlflow_deployment import MLFlowDeploymentService

# DEPLOY = "deploy"
# PREDICT = "predict"
# DEPLOY_AND_PREDICT = "deploy_and_predict"


# @click.command()
# @click.option(
#     "--config",
#     "-c",
#     type=click.Choice([DEPLOY, PREDICT, DEPLOY_AND_PREDICT]),
#     default=DEPLOY_AND_PREDICT,
#     help="Optionally you can choose to only run the dpeoyment pipeline to train and deploy a model (deploy) or to only run a prediction against the deplayed model (Predict) the dault is both will be run (deploy and predict)",
# )
# @click.option(
#     "--min-accuracy", default=0.92, help="minimun accuracy required to deploy the model"
# )
# def run_deployment(config: str, min_accuracy: float):
#     mlflow_model_deployer_component = MLFlowModelDeployer.get_active_model_deployer()
#     deploy = config == DEPLOY or config == DEPLOY_AND_PREDICT
#     predict = config == PREDICT or config == DEPLOY_AND_PREDICT

#     if deploy:
#         continous_deployment_pipeline(
#             data_path=r"C:\Users\reyde\Desktop\Coding Project\MLOps\Data\olist_customers_dataset.csv",
#             min_accuracy=min_accuracy,
#             workers=3,
#             timeout=60,
#         )
#     if predict:
#         inference_pipeline(
#             pipeline_name="continous_deployement_pipeline",
#             pipeline_step_name="mlflow_modle_deployer_step",
#         )

#     print(
#         "you can run: \n "
#         f"[italic green]    mlflow ui --backend-store-uri '{get_tracking_uri}'"
#         "[/italic green]\n ...to inspect your experiment runs within the MLflow"
#         "UI. \n You can find your runs tracked within the mlflow_example_pipeline experiment. There you will also be anleto compare two or more runs. \n\n"
#     )

#     # Fetch existing services with the same pipeline name, step name, and model name
#     existin_service = mlflow_model_deployer_component.find_model_server(
#         pipeline_name="continuous_deployment_pipeline",
#         pipeline_step_name="mlflow_model_deployer_step",
#         model_name="model",
#     )
#     if existin_service:
#         service = cast(MLFlowDeploymentService, existin_service[0])
#         if service.is_running:
#             print(
#                 "the mlflow prediction server running locally"
#                 "accepts inference requests at"
#                 f"{service.prediction_url}\n"
#                 f"to stop the service run [italic green]"
#                 f"zenml model-deployer models delete {str(service.uuid)} [/italic green]"
#             )

#         elif service.is_failed:
#             print(
#                 f"The mlflow prediction server is in a failed state: \n"
#                 f"Last state: '{service.status.state.value}'\n"
#                 f"Last Error: '{service.status.last_error}'"
#             )

#     else:
#         print(
#             "no mlfolow prediction srver is runneing. the deployment pipeline must run and train a model then deploy it."
#             "execute th esame command with the --deplo argument to deploy a model"
#         )


# if __name__ == "__main__":
#     run_deployment()
